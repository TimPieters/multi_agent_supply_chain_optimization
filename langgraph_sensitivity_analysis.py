# LangGraph implementation for automated sensitivity analysis of the CFLP model

import json
import operator
import pandas as pd
from datetime import datetime # Import datetime for unique run IDs
import time
import os # Import os for path manipulation and directory creation
from pathlib import Path # Import Path for path manipulation
from itertools import product # Import product for generating combinations

from typing import TypedDict, Annotated, List, Union
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langgraph.graph import StateGraph, END
from langchain.callbacks.manager import get_openai_callback

from utils import _read_source_code, modify_and_run_model
from coder_agent import CoderAgent
from config import MODEL_FILE_PATH, MODEL_DATA_PATH, MODEL_DESCRIPTION_PATH, DATA_BOUNDS, MODEL_PARAMETERS

# --- Configuration ---
# Paths are now loaded from config.py
LOG_FILE = "logs/agent_execution_log.json" # Log file used by existing agents
BASELINE_OBJ = 366.10 # Baseline objective value for comparison (consider moving to config or dynamic calculation)

# Global variable for the base log directory, can be changed dynamically
BASE_LOG_DIR = "logs"

def set_log_directory(base_log_dir: str):
    """Sets the base directory for all log files and ensures it exists."""
    global BASE_LOG_DIR
    BASE_LOG_DIR = base_log_dir
    os.makedirs(BASE_LOG_DIR, exist_ok=True)
    print(f"Log directory set to: {BASE_LOG_DIR}")

# Dynamically load model source code, input data, and model description
source_code = _read_source_code(MODEL_FILE_PATH)
input_data = _read_source_code(MODEL_DATA_PATH)
model_description = _read_source_code(MODEL_DESCRIPTION_PATH)

# --- State Definition ---

class SensitivityAnalysisState(TypedDict):
    """
    Represents the state of the sensitivity analysis graph.

    Attributes:
        baseline_objective: The objective value of the original model.
        scenario_log: A list of strings summarizing past scenarios and their results.
        proposed_scenario: The natural language description of the scenario to test next.
        code_modification: The JSON modification generated by the Coder Agent.
        execution_result: The result dictionary from running the modified model.
        max_iterations: The maximum number of scenarios to run.
        current_iteration: The current iteration count.
        error_message: Stores any error messages encountered during execution.
    final_analysis_summary: str # Added field for final summary
    token_usage: dict # Added field for storing token usage per agent
    run_id: str # Unique identifier for each full sensitivity run
    planner_model: str # Model used by the planner LLM
    planner_temperature: float # Temperature used by the planner LLM
    coder_model: str # Model used by the coder LLM
    coder_temperature: float # Temperature used by the coder LLM
    final_analysis_model: str # Model used by the final analysis LLM
    final_analysis_temperature: float # Temperature used by the final analysis LLM
    scenario_start_time: float # Timestamp for the start of the current scenario
    total_run_time: float # Total time for the entire sensitivity analysis run
    """
    baseline_objective: float
    scenario_log: List[str]
    proposed_scenario: str
    code_modification: Union[dict, str] # Can be dict or JSON string
    execution_result: dict
    max_iterations: int
    current_iteration: int
    error_message: str
    token_usage: dict # Added field for storing token usage per agent
    final_analysis_summary: str # Added field for final summary
    run_id: str # Unique identifier for each full sensitivity run
    planner_model: str # Model used by the planner LLM
    planner_temperature: float # Temperature used by the planner LLM
    coder_model: str # Model used by the coder LLM
    coder_temperature: float # Temperature used by the coder LLM
    final_analysis_model: str # Model used by the final analysis LLM
    final_analysis_temperature: float # Temperature used by the final analysis LLM
    scenario_start_time: float # Timestamp for the start of the current scenario
    total_run_time: float # Total time for the entire sensitivity analysis run

# --- Nodes (Agent Functions) ---

# LLM instances will be initialized dynamically within run_sensitivity_analysis

# 1. Planner Node
planner_prompt_template = """
You are a scenario planning AI for supply chain optimization.
Your job is to intelligently explore the sensitivity of the optimization model.
You are *not* just suggesting random changes — your goal is to find scenarios that cause a *large impact* on the total cost or feasibility.
Given past results, propose a scenario likely to have higher impact.

--- MODEL CONTEXT ---
{model_description}

--- MODEL INPUT DATA ---
{input_data}

--- PAST SCENARIOS LOG ---
{scenario_log}

Review the scenarios and their objective function impacts (ΔObj). Then propose ONE new scenario that is likely to:
- Significantly increase or decrease the total cost
- Lead to infeasibility
- Reveal something critical about model behavior

Do NOT repeat similar changes (e.g., repeatedly changing the same parameter type).
Avoid low-impact changes like small cost tweaks.

Propose the scenario as a single line of natural language.

--- NEW SCENARIO PROPOSAL ---
"""

planner_prompt = PromptTemplate(
    input_variables=["model_description", "scenario_log","input_data"],
    template=planner_prompt_template
)

# planner_chain will be initialized dynamically within run_sensitivity_analysis

def planner_node(state: SensitivityAnalysisState) -> SensitivityAnalysisState:
    """Proposes the next sensitivity scenario."""
    print("--- Running Planner Node ---")
    state['scenario_start_time'] = time.time() # Record start time for this scenario
    try:
        scenario_log_str = "\n".join(state['scenario_log'])
        # Wrap in token callback
        with get_openai_callback() as cb:
            proposed_scenario = planner_chain.invoke({
                "model_description": model_description,
                "scenario_log": scenario_log_str,
                "input_data": input_data
            })
        # Log usage
        usage = {'total': cb.total_tokens, 'prompt': cb.prompt_tokens, 'completion': cb.completion_tokens, 'total_cost': cb.total_cost}
        state.setdefault('token_usage', {})['planner'] = usage
        print(f"Planner token usage: {usage}")

        print(f"Proposed Scenario: {proposed_scenario}")
        state['proposed_scenario'] = proposed_scenario.strip()
        state['error_message'] = None # Clear previous errors
    except Exception as e:
        print(f"Error in Planner Node: {e}")
        state['error_message'] = f"Planner Error: {e}"
        # Decide how to handle error - maybe end graph? For now, store message.
    return state

# 2. Coder Node (Adapting CoderAgent logic)
# We'll instantiate CoderAgent within the node or reuse its core logic.
# For simplicity here, let's assume CoderAgent's run method can be called directly.
# Note: CoderAgent uses a ReAct approach internally, which might be overkill here.
# A simpler chain might suffice: Prompt -> LLM -> JSON Parser
# Let's try a direct call first.

# Simplified approach: Use a dedicated LLM call for translation
# coder_llm will be initialized dynamically within run_sensitivity_analysis

coder_prompt_template = """
Translate the following natural language scenario for a supply chain optimization model into a JSON modification suitable for the 'modify_and_run_model' tool.
The JSON should have one key: "ADD DATA" or "ADD CONSTRAINT".
The value should be a Python string representing the code to modify the parameter lists/arrays in the '{model_file_path}' file, or add a constraint using 'model += ...'.

You are an AI assistant for supply chain optimization. You analyze the provided Python optimization model
and modify it based on the user's questions. You just state your added code.
You use the key "ADD CONSTRAINT" to add a constraint, and you use the key "ADD DATA" to add data.
You can only do one modification.

You must provide a valid JSON object using double quotes for both keys and values. NEVER add ```json ```
Example:
{{ "ADD CONSTRAINT": "model += lpSum(variables[0, j] for j in range(len(demand))) <= 80, \\"Supply_Limit_Supplier_0\\"" }}
or
{{ "ADD DATA": "supply = [200, 300, 300]" }}
Do not use single quotes or Python-style dictionaries.

your written code will be added to the line with substring:
"### DATA MANIPULATION CODE HERE ###"
"### CONSTRAINT CODE HERE ###"

Below is the full source code of the supply chain model:
---SOURCE CODE---
```python
{source_code}
```

--- Input Data ---
{input_data}
---

Focus on modifying the existing data structures based on the scenario.
Example Scenario: "Increase all demands by 15%"
Example JSON Output: {{ "ADD DATA": "demands = [int(d * 1.15) for d in demands]" }}

Example Scenario: "Decrease fixed costs by 10%"
Example JSON Output: {{ "ADD DATA": "fixed_costs = [fc * 0.90 for fc in fixed_costs]" }}

Example Scenario: "Limit capacity of facility 2 (index 1) to 60"
Example JSON Output: {{ "ADD DATA": "capacities[1] = 60" }}

Example Scenario: "Add a constraint that total items served by facility 3 (index 2) must be at least 50"
Example JSON Output: {{ "ADD CONSTRAINT": "model += lpSum(assignment[i, 2] * demands[i] for i in range(n_customers)) >= 50, 'MinServe_Fac2'" }}


Scenario: "{scenario}"

JSON Output:
"""
coder_prompt = PromptTemplate(
    input_variables=["scenario", "source_code", "input_data", "model_file_path"],
    template=coder_prompt_template
)

def coder_node(state: SensitivityAnalysisState) -> SensitivityAnalysisState:
    """Translates the natural language scenario into code modifications."""
    print("--- Running Coder Node ---")
    if state.get('error_message'): # Skip if previous node failed
        print("Skipping Coder Node due to previous error.")
        return state
    try:
        scenario = state['proposed_scenario']
        if not scenario:
            raise ValueError("No scenario proposed by the planner.")

        # CoderAgent's run method expects a string and returns the result dict.
        # We need the 'final_answer' which should be the JSON modification.
        # Note: CoderAgent's run also executes the model, which we want to separate.
        # Let's call the internal agent executor directly to get the thought process
        # and extract the final JSON action input without executing.

        # coder_llm is now dynamically initialized in run_sensitivity_analysis,
        # and coder_chain is re-created there.
        # So, we just use the global coder_chain here.
        with get_openai_callback() as cb:
            json_modification_str = coder_chain.invoke({
                "scenario": scenario,
                "source_code": source_code,
                "input_data": input_data,
                "model_file_path": MODEL_FILE_PATH # Pass the dynamic model file path
            })

        # Log usage
        usage = {'total': cb.total_tokens, 'prompt': cb.prompt_tokens, 'completion': cb.completion_tokens, 'total_cost': cb.total_cost}
        state.setdefault('token_usage', {})['coder'] = usage
        print(f"Coder token usage: {usage}")

        # Basic validation/cleaning of the JSON string
        json_modification_str = json_modification_str.strip().strip('`')
        if json_modification_str.startswith("json"):
             json_modification_str = json_modification_str[4:].strip()

        print(f"Generated Code Modification (String): {json_modification_str}")

        # Try parsing the JSON
        try:
            code_modification = json.loads(json_modification_str)
            # Further validation (optional): check keys, value types
            if not isinstance(code_modification, dict) or not any(k in code_modification for k in ["ADD DATA", "ADD CONSTRAINT"]):
                 raise ValueError("Invalid JSON structure. Must be a dict with 'ADD DATA' or 'ADD CONSTRAINT'.")
            print(f"Parsed Code Modification (Dict): {code_modification}")
            state['code_modification'] = code_modification # Store the parsed dict
        except json.JSONDecodeError as e:
            print(f"JSON Decode Error: {e}. Raw output: {json_modification_str}")
            # Store the raw string if parsing fails, let the execution node handle it or raise error
            state['code_modification'] = json_modification_str # Store raw string on error
            raise ValueError(f"Failed to parse JSON from Coder LLM: {e}")
        except ValueError as ve:
             print(f"JSON Validation Error: {ve}. Raw output: {json_modification_str}")
             state['code_modification'] = json_modification_str # Store raw string on error
             raise ve


        state['error_message'] = None
    except Exception as e:
        print(f"Error in Coder Node: {e}")
        state['error_message'] = f"Coder Error: {e}"
    return state

# 3. Execute Node
def execute_node(state: SensitivityAnalysisState) -> SensitivityAnalysisState:
    """Executes the modified model."""
    print("--- Running Execute Node ---")
    if state.get('error_message'):
        print("Skipping Execute Node due to previous error.")
        return state
    try:
        modification = state['code_modification']
        if not modification:
            raise ValueError("No code modification provided by the coder.")

        # modify_and_run_model expects a dict, but handle string case from potential coder error
        if isinstance(modification, str):
             try:
                 modification = json.loads(modification)
             except json.JSONDecodeError:
                 raise ValueError(f"Invalid JSON string received for modification: {modification}")

        print(f"Executing model with modification: {modification}")
        # Pass the FULL_LOG_PATH to modify_and_run_model
        result = modify_and_run_model(
            modification,
            MODEL_FILE_PATH,
            MODEL_DATA_PATH,
            run_id=f"run_id_{state['run_id']}_it_{state['current_iteration']}", # Unique run ID for this iteration
            log_filepath=os.path.join(BASE_LOG_DIR, f"run_log_{state['run_id']}.csv") # Pass the specific log file path
        )
        print(f"Execution Result: {result}")
        state['execution_result'] = result
        state['error_message'] = None

        # Check for execution errors reported by the tool
        if isinstance(result, str) and ("Error" in result or "error" in result):
             state['error_message'] = f"Execution Error: {result}"
        elif isinstance(result, dict) and result.get("status") not in ["Optimal", "Not Solved"]: # PuLP statuses
             # Handle non-optimal statuses if needed, e.g., Infeasible, Unbounded
             print(f"Warning: Model solved with status: {result.get('status')}")
             # Optionally treat infeasible as an error or just log it
             if result.get("status") == "Infeasible":
                  state['error_message'] = "Execution Result: Model Infeasible"


    except Exception as e:
        print(f"Error in Execute Node: {e}")
        state['error_message'] = f"Execute Error: {e}"
        state['execution_result'] = {"status": "Execution Failed", "error": str(e)}
    return state

# 4. Analyze Node
def analyze_node(state: SensitivityAnalysisState) -> SensitivityAnalysisState:
    """Analyzes the result, updates the log, and increments iteration."""
    print("--- Running Analyze Node ---")
    if state.get('error_message') and "Execution Error" not in state['error_message'] and "Model Infeasible" not in state['error_message']:
        # If error is from Planner or Coder, don't analyze, just pass the error
        print("Skipping Analyze Node due to Planner/Coder error.")
        return state

    current_iteration = state.get('current_iteration', 0)
    baseline_obj = state['baseline_objective']
    scenario = state['proposed_scenario']
    code_mod = state.get('code_modification')
    result = state['execution_result']
    log = state.get('scenario_log', [])

    # Format the log entry based on the result
    log_entry = f"{current_iteration + 1}. Scenario: '{scenario}' -> "
    if state.get('error_message'):
         log_entry += f"Error: {state['error_message']}"
    elif result and isinstance(result, dict):
        status = result.get('status', 'Unknown Status')
        if status == 'Optimal':
            cost = result.get('total_cost', None)
            if cost is not None and baseline_obj is not None and baseline_obj != 0:
                delta = round(cost - baseline_obj, 2)
                delta_pct = round((delta / baseline_obj) * 100, 1)
                log_entry += f"Status: Optimal, Cost: {round(cost, 2)}, ΔObj: {delta} ({delta_pct}%)"
            elif cost is not None:
                log_entry += f"Status: Optimal, Cost: {round(cost, 2)}"
            else:
                log_entry += "Status: Optimal, Cost: N/A"
        else:
            log_entry += f"Status: {status}"
            # Capture specific messages for infeasibility etc. if present
            if "message" in result:
                 log_entry += f" ({result['message']})"
    else:
        log_entry += f"Execution Failed or Invalid Result Format: {result}"

    print(f"Log Entry: {log_entry}")
    log.append(log_entry)
    state['scenario_log'] = log

    # ─── Detailed internal CSV logging via util log ────────────────────
    # Construct a unique log file path for each run
    run_id = state['run_id']
    # Use the global BASE_LOG_DIR
    FULL_LOG_PATH = os.path.join(BASE_LOG_DIR, f'run_log_{run_id}.csv')

    # Read the most recent entry from the FULL_LOG_PATH (logged by modify_and_run_model)
    # This read is to get the base data from the model execution, not the full log.
    try:
        full_df_from_util_log = pd.read_csv(FULL_LOG_PATH)
        if full_df_from_util_log.empty:
            raise ValueError("Run log file is empty. Expected a record from modify_and_run_model.")
        
        # Get the index of the last entry
        last_entry_idx = full_df_from_util_log.index[-1]
        # Get the last entry as a dictionary
        last_entry = full_df_from_util_log.iloc[last_entry_idx].to_dict()
    except FileNotFoundError:
        print(f"Error: Run log file not found at {FULL_LOG_PATH}. Cannot enrich entry.")
        return state # Return state if file not found, as there's nothing to update
    except Exception as e:
        print(f"Error reading last entry from run log: {e}")
        return state # Return state on other errors

    # Build enriched record
    record = {}
    # Copy all util log fields
    for k, v in last_entry.items():
        record[k] = v
    # Add agent metadata
    record['scenario_text'] = scenario
    record['code_modification'] = json.dumps(code_mod, ensure_ascii=False)
    record['planner_model'] = state['planner_model']
    record['planner_temperature'] = state['planner_temperature']
    record['coder_model'] = state['coder_model']
    record['coder_temperature'] = state['coder_temperature']

    # Calculate and add scenario duration
    scenario_duration = time.time() - state['scenario_start_time']
    record['agent_execution_time'] = round(scenario_duration, 4) # Log duration to 4 decimal places

    # Parse JSON columns back to Python objects if needed
    for col in ['parameters', 'constraints', 'variables']:
        if col in record and isinstance(record[col], str): # Ensure it's a string before trying to load JSON
            try:
                record[col] = json.loads(record[col])
            except Exception:
                pass # Keep as string if parsing fails

    # Compute delta_obj if baseline and objective present
    if 'objective_value' in record and baseline_obj is not None:
        try:
            record['delta_obj'] = round(record['objective_value'] - baseline_obj, 6)
        except Exception:
            record['delta_obj'] = None

    record['token_usage'] = json.dumps(state.get('token_usage', {}), ensure_ascii=False)

    # Update the last row of the DataFrame with the enriched 'record'
    # This will update existing columns and add new ones if they don't exist
    for k, v in record.items():
        full_df_from_util_log.at[last_entry_idx, k] = v

    # Save the updated DataFrame (always write the full DataFrame, overwriting the old one)
    full_df_from_util_log.to_csv(FULL_LOG_PATH, index=False)
    print(f"Enriched run entry saved to {FULL_LOG_PATH}")

    # ─────────────────────────────────────────────────────────────────────


    state['current_iteration'] = current_iteration + 1
    # Clear execution-specific fields for the next loop
    state['proposed_scenario'] = None
    state['code_modification'] = None
    state['execution_result'] = None
    # Keep error message if it occurred, otherwise clear it
    # state['error_message'] = None # Let's keep the error message for the decision node

    return state


# Helper function to print the graph
def print_workflow_graph(workflow):
    """Prints the LangGraph workflow as a Mermaid diagram for Jupyter Notebooks."""
    try:
        # Compile the workflow temporarily to get the graph for printing
        temp_app = workflow.compile()
        mermaid_diagram = temp_app.get_graph().draw_mermaid()
        print("```mermaid")
        print(mermaid_diagram)
        print("```")
    except Exception as e:
        print(f"Error generating or printing Mermaid diagram: {e}")


# 5. Final Analyzer Node
final_analyzer_llm = ChatOpenAI(model="gpt-4o", temperature=0.1) # Use a powerful model for analysis

final_analyzer_prompt_template = """
You are a Supply Chain Sensitivity Analysis AI.
You have been provided with a log of scenarios tested against a supply chain optimization model and their results.
Your task is to analyze this log and provide a summary of the findings, focusing on identifying which types of changes or parameters had the most significant impact on the total cost (ΔObj) or model feasibility (e.g., causing Infeasibility).

--- MODEL DESCRIPTION ---
{model_description}

--- SCENARIO LOG ---
{scenario_log}
--- END SCENARIO LOG ---

Based *only* on the information in the log, perform the following:
1.  **Summarize Key Findings:** Briefly describe the overall sensitivity observed. Were there specific types of changes (e.g., demand increases, capacity limits, cost changes, new constraints) that consistently caused large impacts?
2.  **Rank Parameter/Change Type Sensitivity:** Provide a ranked list of the parameter types or kinds of changes based on their observed impact. Start with the most impactful. Consider both the magnitude of cost changes (ΔObj %) and instances of infeasibility. Be specific about the parameter (e.g., 'Demand Levels', 'Facility Capacities', 'Fixed Costs', 'Transportation Costs', 'Specific Constraints').
3.  **Identify Critical Scenarios:** Mention any specific scenarios from the log that were particularly revealing (e.g., caused the largest cost swing, unexpectedly led to infeasibility).

Format your output clearly.

--- ANALYSIS SUMMARY ---
"""

final_analyzer_prompt = PromptTemplate(
    input_variables=["scenario_log", "model_description"],
    template=final_analyzer_prompt_template
)

# final_analyzer_chain will be initialized dynamically within run_sensitivity_analysis

def final_analyzer_node(state: SensitivityAnalysisState) -> SensitivityAnalysisState:
    """Analyzes the complete scenario log to summarize sensitivity."""
    print("--- Running Final Analyzer Node ---")
    try:
        scenario_log_str = "\n".join(state['scenario_log'])
        if not scenario_log_str:
            summary = "No scenarios were run or logged."
        else:
            with get_openai_callback() as cb:
                summary = final_analyzer_chain.invoke({"scenario_log": scenario_log_str,"model_description": model_description})
            # Log usage
            usage = {'total': cb.total_tokens, 'prompt': cb.prompt_tokens, 'completion': cb.completion_tokens, 'total_cost': cb.total_cost}
            state.setdefault('token_usage', {})['final_analyzer'] = usage
            print(f"Final Analyzer token usage: {usage}")

        print("Generated Final Analysis Summary:")
        print(summary)
        state['final_analysis_summary'] = summary
        state['final_analysis_model'] = final_analyzer_llm.model_name
        state['final_analysis_temperature'] = final_analyzer_llm.temperature
    except Exception as e:
        print(f"Error in Final Analyzer Node: {e}")
        # Store error in the summary field or a dedicated error field if preferred
        state['final_analysis_summary'] = f"Error during final analysis: {e}"
        state['error_message'] = f"Final Analysis Error: {e}" # Also update main error message

    return state


# --- Conditional Edges ---

def should_continue(state: SensitivityAnalysisState) -> str:
    """Determines whether to continue the analysis or end."""
    print("--- Checking Condition: Should Continue? ---")
    current_iteration = state['current_iteration']
    max_iterations = state['max_iterations']
    error_message = state.get('error_message')

    if error_message and "Execution Error" not in error_message and "Model Infeasible" not in error_message:
        # If Planner or Coder failed critically, stop.
        print(f"Stopping due to critical error: {error_message}")
        return "end_run"
    elif current_iteration >= max_iterations:
        print(f"Stopping: Reached max iterations ({max_iterations}).")
        return "end_run"
    else:
        print(f"Continuing: Iteration {current_iteration + 1}/{max_iterations}.")
        return "continue_run"

# --- Build the Graph ---

workflow = StateGraph(SensitivityAnalysisState)

# Add nodes
workflow.add_node("planner", planner_node)
workflow.add_node("coder", coder_node)
workflow.add_node("executor", execute_node)
workflow.add_node("analyzer", analyze_node)
workflow.add_node("final_analyzer", final_analyzer_node) # Add the final analyzer node

# Define edges
workflow.set_entry_point("planner")
workflow.add_edge("planner", "coder")
workflow.add_edge("coder", "executor")
workflow.add_edge("executor", "analyzer")

# Add conditional edge
workflow.add_conditional_edges(
    "analyzer",
    should_continue,
    {
        "continue_run": "planner",
        "end_run": "final_analyzer", # Route to final analyzer instead of END
    },
)
workflow.add_edge("final_analyzer", END) # Add edge from final analyzer to END


def run_sensitivity_analysis(
    baseline_objective: float,
    max_iterations: int,
    planner_model: str,
    planner_temperature: float,
    coder_model: str,
    coder_temperature: float,
    model_file_path: str,
    model_data_path: str,
    model_description_path: str,
    openai_api_key: str = None, # Added openai_api_key parameter
    stream_output: bool = False # Added parameter to control streaming output
):
    """
    Runs the LangGraph sensitivity analysis with specified parameters.
    """
    global source_code, input_data, model_description, BASELINE_OBJ, MODEL_FILE_PATH, MODEL_DATA_PATH, MODEL_DESCRIPTION_PATH
    global planner_chain, coder_chain, final_analyzer_chain, app # Declare app as global to recompile

    # Update global variables for the current run (these are for logging/metadata)
    MODEL_FILE_PATH = model_file_path
    MODEL_DATA_PATH = model_data_path
    MODEL_DESCRIPTION_PATH = model_description_path
    BASELINE_OBJ = baseline_objective

    # Re-initialize LLMs with potentially new models/temperatures and API key
    if openai_api_key:
        planner_llm_dynamic = ChatOpenAI(model=planner_model, temperature=planner_temperature, openai_api_key=openai_api_key)
        coder_llm_dynamic = ChatOpenAI(model=coder_model, temperature=coder_temperature, openai_api_key=openai_api_key)
        final_analyzer_llm_dynamic = ChatOpenAI(model="gpt-4o", temperature=0.1, openai_api_key=openai_api_key) # Final analyzer model/temp fixed or passed as param
    else:
        # Fallback to environment variable if key not provided
        planner_llm_dynamic = ChatOpenAI(model=planner_model, temperature=planner_temperature)
        coder_llm_dynamic = ChatOpenAI(model=coder_model, temperature=coder_temperature)
        final_analyzer_llm_dynamic = ChatOpenAI(model="gpt-4o", temperature=0.1) # Final analyzer model/temp fixed or passed as param

    # Re-create chains with the new LLM instances
    planner_chain = planner_prompt | planner_llm_dynamic | StrOutputParser()
    coder_chain = coder_prompt | coder_llm_dynamic | StrOutputParser()
    final_analyzer_chain = final_analyzer_prompt | final_analyzer_llm_dynamic | StrOutputParser()

    # Recompile the graph to use the new chains
    app = workflow.compile()

    # Generate a unique run ID based on timestamp
    run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    print(f"Starting new sensitivity run with ID: {run_id}")

    initial_state = SensitivityAnalysisState(
        baseline_objective=BASELINE_OBJ,
        scenario_log=[],
        proposed_scenario=None,
        code_modification=None,
        execution_result=None,
        max_iterations=max_iterations,
        current_iteration=0,
        error_message=None,
        run_id=run_id,
        planner_model=planner_llm_dynamic.model_name,
        planner_temperature=planner_llm_dynamic.temperature,
        coder_model=coder_llm_dynamic.model_name,
        coder_temperature=coder_llm_dynamic.temperature,
        final_analysis_model=final_analyzer_llm_dynamic.model_name, # Initialize final_analysis_model
        final_analysis_temperature=final_analyzer_llm_dynamic.temperature, # Initialize final_analysis_temperature
        total_run_time=0.0,
    )

    # Run the graph and measure total execution time
    start_total_time = time.time()
    if stream_output:
        print("\n🔄 Starting streaming analysis...")
        print("=" * 60)
        
        final_state = None
        
        # Stream the execution
        for event in app.stream(initial_state, config={"recursion_limit": 200}):
            # event is a dictionary with node names as keys and their outputs as values
            for node_name, node_output in event.items():
                print(f"\n📍 Node: {node_name.upper()}")
                print("-" * 40)
                
                # Display relevant information based on the node
                if node_name == "planner":
                    if node_output.get('proposed_scenario'):
                        print(f"💡 Proposed Scenario: {node_output['proposed_scenario']}")
                    print(f"🔄 Iteration: {node_output.get('current_iteration', 'N/A')}")
                    
                elif node_name == "coder":
                    if node_output.get('code_modification'):
                        print(f"🔧 Code Modification Generated")
                        # Optionally show a snippet of the code modification
                        code_mod = node_output['code_modification']
                        if len(code_mod) > 200:
                            print(f"📝 Code Preview: {code_mod[:200]}...")
                        else:
                            print(f"📝 Code: {code_mod}")
                    
                elif node_name == "executor":
                    if node_output.get('execution_result'):
                        result = node_output['execution_result']
                        print(f"⚡ Execution Result: {result}")
                    if node_output.get('error_message'):
                        print(f"❌ Error: {node_output['error_message']}")
                        
                elif node_name == "analyzer":
                    print(f"📊 Analysis Complete for Iteration {node_output.get('current_iteration', 'N/A')}")
                    if len(node_output.get('scenario_log', [])) > 0:
                        print(f"📝 Total Scenarios Logged: {len(node_output['scenario_log'])}")
                        
                elif node_name == "final_analyzer":
                    print("🎯 FINAL ANALYSIS COMPLETE")
                    if node_output.get('final_analysis_summary'):
                        print(f"📋 Summary Available: {len(node_output['final_analysis_summary'])} characters")
                
                # Always show current state info
                print(f"⏱️  Current Runtime: {time.time() - start_total_time:.2f}s")
                
                # Store the final state
                final_state = node_output
                
        print("\n" + "=" * 60)
        print("🎉 Streaming complete!")
        
    else:
        # Original non-streaming behavior
        final_state = app.invoke(initial_state, config={"recursion_limit": 100})
    end_total_time = time.time()
    total_run_duration = end_total_time - start_total_time

    # Update the final state with the total run time
    final_state['total_run_time'] = total_run_duration

    print("\n--- Analysis Complete ---")
    print("Final State:")
    # Pretty print the final state
    print(json.dumps(final_state, indent=2, default=str))

    # Dynamically parse input_data to get parameter count based on MODEL_PARAMETERS in config
    try:
        parsed_input_data = json.loads(input_data)
        # Removed 'import os' as it's now global

        current_model_filename = MODEL_FILE_PATH
        model_specific_params = MODEL_PARAMETERS.get(current_model_filename, [])
        
        total_parameters = 0
        for param_name in model_specific_params:
            param_value = parsed_input_data.get(param_name)
            if isinstance(param_value, (list, tuple)):
                # For lists/tuples, count each element as a parameter
                total_parameters += len(param_value)
            elif isinstance(param_value, (int, float)):
                # For single numerical values, count as one parameter
                total_parameters += 1
            # Add more types if needed (e.g., dictionaries, nested lists)
    except Exception as e:
        print(f"Error parsing input data for parameter count: {e}")
        total_parameters = "N/A"

    # Save the final log with run_id and additional details
    log_output_path = os.path.join(BASE_LOG_DIR, f"scenario_log_{run_id}.txt")
    with open(log_output_path, "w") as f:
        f.write(f"LangGraph Automated Sensitivity Analysis Log (Run ID: {run_id})\n")
        f.write("="*60 + "\n\n")
        f.write(f"Model Path: {MODEL_FILE_PATH}\n")
        f.write(f"Data Path: {MODEL_DATA_PATH}\n")
        f.write(f"Baseline Objective Value: {BASELINE_OBJ}\n")
        f.write(f"Number of Parameters: {total_parameters}\n")
        f.write("\n") # Added a newline for better formatting
        f.write(f"Iterations Ran: {final_state['current_iteration']}\n")
        f.write(f"Total Run Time: {final_state.get('total_run_time', 0.0):.2f} seconds\n")
        f.write(f"Planner LLM: {final_state.get('planner_model', 'N/A')} (Temp: {final_state.get('planner_temperature', 'N/A')})\n")
        f.write(f"Coder LLM: {final_state.get('coder_model', 'N/A')} (Temp: {final_state.get('coder_temperature', 'N/A')})\n")
        f.write(f"Final Analysis LLM: {final_state.get('final_analysis_model', 'N/A')} (Temp: {final_state.get('final_analysis_temperature', 'N/A')})\n\n")
        f.write("--- Final Analysis Summary ---\n")
        f.write(final_state.get('final_analysis_summary', 'No summary available.') + "\n\n")
        f.write("\n")
        f.write("--- Scenario Log ---\n")
        for entry in final_state.get('scenario_log', []):
            f.write(entry + "\n")
    print(f"\nScenario log saved to {log_output_path}")

    return final_state

# Alternative: If you want a separate streaming function that yields results
def run_sensitivity_analysis_generator(
    baseline_objective: float,
    max_iterations: int,
    planner_model: str,
    planner_temperature: float,
    coder_model: str,
    coder_temperature: float,
    model_file_path: str,
    model_data_path: str,
    model_description_path: str,
    openai_api_key: str = None,
):
    """
    Generator version that yields intermediate results for custom handling.
    """
    global source_code, input_data, model_description, BASELINE_OBJ, MODEL_FILE_PATH, MODEL_DATA_PATH, MODEL_DESCRIPTION_PATH
    global planner_chain, coder_chain, final_analyzer_chain, app # Ensure app is global for recompilation

    # Setup code (same as above)
    MODEL_FILE_PATH = model_file_path
    MODEL_DATA_PATH = model_data_path
    MODEL_DESCRIPTION_PATH = model_description_path
    BASELINE_OBJ = baseline_objective

    # Re-read source code, input data, and model description in case paths changed
    source_code = _read_source_code(MODEL_FILE_PATH)
    input_data = _read_source_code(MODEL_DATA_PATH)
    model_description = _read_source_code(MODEL_DESCRIPTION_PATH)

    # Re-initialize LLMs with potentially new models/temperatures and API key
    if openai_api_key:
        planner_llm_dynamic = ChatOpenAI(model=planner_model, temperature=planner_temperature, openai_api_key=openai_api_key)
        coder_llm_dynamic = ChatOpenAI(model=coder_model, temperature=coder_temperature, openai_api_key=openai_api_key)
        final_analyzer_llm_dynamic = ChatOpenAI(model="gpt-4o", temperature=0.1, openai_api_key=openai_api_key)
    else:
        planner_llm_dynamic = ChatOpenAI(model=planner_model, temperature=planner_temperature)
        coder_llm_dynamic = ChatOpenAI(model=coder_model, temperature=coder_temperature)
        final_analyzer_llm_dynamic = ChatOpenAI(model="gpt-4o", temperature=0.1)

    # Re-create chains with the new LLM instances
    planner_chain = planner_prompt | planner_llm_dynamic | StrOutputParser()
    coder_chain = coder_prompt | coder_llm_dynamic | StrOutputParser()
    final_analyzer_chain = final_analyzer_prompt | final_analyzer_llm_dynamic | StrOutputParser()

    # Recompile the graph to use the new chains
    app = workflow.compile()

    run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    initial_state = SensitivityAnalysisState(
        baseline_objective=BASELINE_OBJ,
        scenario_log=[],
        proposed_scenario=None,
        code_modification=None,
        execution_result=None,
        max_iterations=max_iterations,
        current_iteration=0,
        error_message=None,
        run_id=run_id,
        planner_model=planner_llm_dynamic.model_name,
        planner_temperature=planner_llm_dynamic.temperature,
        coder_model=coder_llm_dynamic.model_name,
        coder_temperature=coder_llm_dynamic.temperature,
        total_run_time=0.0,
    )

    start_time = time.time()
    
    # Yield each step
    for event in app.stream(initial_state, config={"recursion_limit": 100}):
        # Add timing information
        current_time = time.time() - start_time
        yield {
            "event": event,
            "elapsed_time": current_time,
            "run_id": run_id
        }

def run_batch_sensitivity_analysis(
    model_file_path: str,
    model_data_path: str,
    model_description_path: str,
    llm_settings_grid: dict,
    max_iterations_per_run: int = 20,
    openai_api_key: str = None
):
    """
    Runs sensitivity analysis for a given model across a grid of LLM settings.
    Logs each run to a dedicated subdirectory.
    """
    print("\n--- Starting Batch Sensitivity Analysis ---")
    
    # Extract model name for directory creation
    model_name = Path(model_file_path).stem.replace("_model", "").upper()
    
    # Extract data configuration from model_data_path
    data_config_name = Path(model_data_path).stem
    # Assuming format like 'capfacloc_data_5cust_5fac'
    # Extracting '_5cust_5fac' part
    data_suffix = ""
    if "capfacloc_data" in data_config_name:
        data_suffix = data_config_name.replace("capfacloc_data", "")
    elif "vrp_data" in data_config_name:
        data_suffix = data_config_name.replace("vrp_data", "")
    
    # Prepare combinations for LLM settings
    planner_models = llm_settings_grid.get("planner_models", ["gpt-4o-mini"])
    planner_temperatures = llm_settings_grid.get("planner_temperatures", [0.7])
    coder_models = llm_settings_grid.get("coder_models", ["gpt-4o"])
    coder_temperatures = llm_settings_grid.get("coder_temperatures", [0.0])

    all_combinations = list(product(
        planner_models, planner_temperatures,
        coder_models, coder_temperatures
    ))

    total_runs = len(all_combinations)
    print(f"Total {total_runs} combinations to run for model: {model_name}")

    batch_results = [] # To collect summary of each batch run

    for i, (p_model, p_temp, c_model, c_temp) in enumerate(all_combinations):
        # Include data_suffix in the run_label
        run_label = f"{model_name}{data_suffix}_P-{p_model}_PT-{p_temp}_C-{c_model}_CT-{c_temp}"
        print(f"\n--- Running Combination {i+1}/{total_runs}: {run_label} ---")

        # Set up a unique log directory for this specific run
        current_run_log_dir = os.path.join("logs", model_name, run_label.replace('.', '_')) # Replace . with _ for valid path
        set_log_directory(current_run_log_dir)

        try:
            # Determine dynamic baseline objective for the current model/data
            print("--- Running Baseline Model to Determine Objective ---")
            baseline_result = modify_and_run_model(
                {}, # No modification for baseline run
                model_file_path=model_file_path,
                model_data_path=model_data_path
            )

            if baseline_result and baseline_result.get("status") == "Optimal":
                dynamic_baseline_obj = baseline_result.get("total_cost")
                if dynamic_baseline_obj is None:
                    raise ValueError("Baseline model solved optimally but returned no total_cost.")
                print(f"Baseline model solved. Objective: {dynamic_baseline_obj:.2f}")
            else:
                raise ValueError(f"Baseline model did not solve optimally. Status: {baseline_result.get('status', 'Unknown')}")

            final_state_for_run = run_sensitivity_analysis(
                baseline_objective=dynamic_baseline_obj,
                max_iterations=max_iterations_per_run,
                planner_model=p_model,
                planner_temperature=p_temp,
                coder_model=c_model,
                coder_temperature=c_temp,
                model_file_path=model_file_path,
                model_data_path=model_data_path,
                model_description_path=model_description_path,
                openai_api_key=openai_api_key,
                stream_output=False # Keep False for batch runs to avoid verbose output
            )
            batch_results.append({
                "run_label": run_label,
                "status": "Completed",
                "final_summary": final_state_for_run.get('final_analysis_summary', 'N/A'),
                "total_run_time": final_state_for_run.get('total_run_time', 'N/A'),
                "iterations_ran": final_state_for_run.get('current_iteration', 'N/A'),
                "planner_model": p_model,
                "planner_temperature": p_temp,
                "coder_model": c_model,
                "coder_temperature": c_temp,
                "log_directory": current_run_log_dir
            })
        except Exception as e:
            print(f"Error running combination {run_label}: {e}")
            batch_results.append({
                "run_label": run_label,
                "status": "Failed",
                "error_message": str(e),
                "log_directory": current_run_log_dir
            })
    
    # Optionally save a summary of all batch runs
    batch_summary_df = pd.DataFrame(batch_results)
    batch_summary_path = os.path.join("logs", model_name, f"{model_name}_batch_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv")
    batch_summary_df.to_csv(batch_summary_path, index=False)
    print(f"\nBatch summary saved to: {batch_summary_path}")
    return batch_summary_df

if __name__ == "__main__":
    print("\n--- LangGraph Workflow Structure (Mermaid) ---")
    # Pass the workflow object to the print function
    print_workflow_graph(workflow)
    print("----------------------------------------------")
    
    # Define the model and data to be used for the batch run
    target_model_file = "models/VRP/vrp_model.py" # Example: CFLP model
    target_data_file = "models/VRP/data/vrp_data_10cust_2veh_50cap.json" # Example: 50 customers, 50 facilities
    target_description_file = "models/VRP/description.txt"

    # Define the grid of LLM settings
    llm_grid_settings = {
        "planner_models": ["gpt-4o-mini", "gpt-4o"], # , "gpt-4.1", "gpt-4.1-mini" #
        "planner_temperatures": [0, 0.5, 1],
        "coder_models": ["gpt-4o-mini", "gpt-4o"], #, "gpt-4.1", "gpt-4.1-mini" #
        "coder_temperatures": [0, 0.5, 1]
    }

    # Run the batch analysis
    batch_summary = run_batch_sensitivity_analysis(
        model_file_path=target_model_file,
        model_data_path=target_data_file,
        model_description_path=target_description_file,
        llm_settings_grid=llm_grid_settings,
        max_iterations_per_run=10, # Reduced for quicker testing
        # openai_api_key="YOUR_OPENAI_API_KEY" # Uncomment and set if not using env var
    )
    print("\n--- Batch execution complete ---")
    print(batch_summary)
